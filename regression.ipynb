{
 "metadata": {
  "name": "",
  "signature": "sha256:86eba3bfa0a614b803cbd8b0e01b45a47305c2b524fe5733a994ffb3bdcd7183"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%pylab inline"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from sklearn.utils import check_random_state\n",
      "\n",
      "def generate_data(n_samples, sigma, random_state=None):\n",
      "    check_random_state(random_state)\n",
      "    x = np.linspace(0, 1, n_samples)\n",
      "    X = x[:, np.newaxis]\n",
      "    y_true = np.cos(2 * np.pi * x)\n",
      "    y = y_true + sigma * np.random.randn(n_samples)\n",
      "    return x, X, y_true, y"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "Linear Regression"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "**Model:** We assume that there is some latent function $f: \\mathbb{R}^D \\rightarrow \\mathbb{R}$. We observe samples $(\\boldsymbol{x}_n, y_n)$ with $f(\\boldsymbol{x}_n) + \\epsilon_n = y_n$. Since we assume that $f$ is a linear function of the form $f(\\boldsymbol{x}_n) = \\boldsymbol{w} \\boldsymbol{x} + b$, we will learn by minimizing\n",
      "\n",
      "$$\\arg \\min_\\boldsymbol{w} \\frac{1}{2} || \\boldsymbol{y} - \\boldsymbol{X} \\boldsymbol{w} ||^2_2.$$"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "$$0 = \\boldsymbol{X}^T \\left( \\boldsymbol{y} - \\boldsymbol{X} \\boldsymbol{w}^T \\right) \\boldsymbol{X} = \\boldsymbol{X}^T \\boldsymbol{y} - \\boldsymbol{X}^T \\boldsymbol{X} \\boldsymbol{w}$$\n",
      "$$\\Leftrightarrow \\boldsymbol{X}^T \\boldsymbol{X} \\boldsymbol{w} = \\boldsymbol{X}^T \\boldsymbol{y}$$\n",
      "$$\\Leftrightarrow \\boldsymbol{w} = \\left( \\boldsymbol{X}^T \\boldsymbol{X} \\right)^{-1} \\boldsymbol{X}^T \\boldsymbol{y}$$"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from sklearn.base import BaseEstimator, RegressorMixin\n",
      "\n",
      "\n",
      "class LinearRegression(BaseEstimator, RegressorMixin):\n",
      "    def fit(self, X, y):\n",
      "        n_samples = X.shape[0]\n",
      "        X_bias = np.hstack((np.ones((n_samples, 1)), X))\n",
      "        self.w = np.linalg.inv(X_bias.T.dot(X_bias)).dot(X_bias.T).dot(y)\n",
      "        return self\n",
      "    def predict(self, X):\n",
      "        n_samples = X.shape[0]\n",
      "        X_bias = np.hstack((np.ones((n_samples, 1)), X))\n",
      "        return X_bias.dot(self.w)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "x, X, y_true, y = generate_data(101, 0.3, 0)\n",
      "plt.plot(x, y_true)\n",
      "plt.scatter(x, y)\n",
      "linreg = LinearRegression().fit(X, y)\n",
      "plt.plot(x, linreg.predict(X))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "Polynomial Regression"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from sklearn.preprocessing import PolynomialFeatures\n",
      "\n",
      "\n",
      "class PolynomialRegression(BaseEstimator, RegressorMixin):\n",
      "    def __init__(self, degree):\n",
      "        self.degree = degree\n",
      "    def fit(self, X, y):\n",
      "        self.poly = PolynomialFeatures(degree=self.degree).fit(X)\n",
      "        X_poly = self.poly.transform(X)\n",
      "        self.w = np.linalg.inv(X_poly.T.dot(X_poly)).dot(X_poly.T).dot(y)\n",
      "        return self\n",
      "    def predict(self, X):\n",
      "        X_poly = self.poly.transform(X)\n",
      "        return X_poly.dot(self.w)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "x, X, y_true, y = generate_data(101, 0.3, 0)\n",
      "plt.plot(x, y_true)\n",
      "plt.scatter(x, y)\n",
      "polyreg = PolynomialRegression(20).fit(X, y)\n",
      "plt.plot(x, polyreg.predict(X))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "Ridge Regression"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "We minimize\n",
      "\n",
      "$$\\arg \\min_\\boldsymbol{w} \\frac{1}{2} || \\boldsymbol{y} - \\boldsymbol{X} \\boldsymbol{w} ||^2_2 + \\frac{\\gamma}{2} \\boldsymbol{w}^T \\boldsymbol{w}.$$"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "$$0\n",
      "= \\boldsymbol{X}^T \\left( \\boldsymbol{y} - \\boldsymbol{X} \\boldsymbol{w}^T \\right) \\boldsymbol{X} + \\gamma \\boldsymbol{w}\n",
      "= \\boldsymbol{X}^T \\boldsymbol{y} - \\boldsymbol{X}^T \\boldsymbol{X} \\boldsymbol{w} + \\gamma \\boldsymbol{w}$$\n",
      "$$\\Leftrightarrow \\boldsymbol{X}^T \\boldsymbol{X} \\boldsymbol{w} + \\gamma \\boldsymbol{w} = \\boldsymbol{X}^T \\boldsymbol{y}$$\n",
      "$$\\Leftrightarrow \\boldsymbol{w} = \\left( \\boldsymbol{X}^T \\boldsymbol{X} + \\gamma \\boldsymbol{I} \\right)^{-1} \\boldsymbol{X}^T \\boldsymbol{y}$$"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from sklearn.preprocessing import PolynomialFeatures\n",
      "\n",
      "\n",
      "class PolynomialRidgeRegression(BaseEstimator, RegressorMixin):\n",
      "    def __init__(self, degree, gamma):\n",
      "        self.degree = degree\n",
      "        self.gamma = gamma\n",
      "    def fit(self, X, y):\n",
      "        self.poly = PolynomialFeatures(degree=self.degree).fit(X)\n",
      "        X_poly = self.poly.transform(X)\n",
      "        self.w = (np.linalg.inv(X_poly.T.dot(X_poly) + self.gamma *\n",
      "                                np.eye(X_poly.shape[1])).dot(X_poly.T).dot(y))\n",
      "        return self\n",
      "    def predict(self, X):\n",
      "        X_poly = self.poly.transform(X)\n",
      "        return X_poly.dot(self.w)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "x, X, y_true, y = generate_data(101, 0.3, 0)\n",
      "plt.plot(x, y_true)\n",
      "plt.scatter(x, y)\n",
      "polyridgereg = PolynomialRidgeRegression(20, 0.1).fit(X, y)\n",
      "plt.plot(x, polyridgereg.predict(X))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "Kernel Regression"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "We can express $\\boldsymbol{w}$ as a weighted sum of the training data\n",
      "$$\\boldsymbol{w} = \\boldsymbol{X}^T \\boldsymbol{\\alpha}$$\n",
      "\n",
      "Which leads to the **dual problem**\n",
      "$$\\arg \\min_\\boldsymbol{\\alpha} \\frac{1}{2} || \\boldsymbol{y} - \\boldsymbol{X} \\boldsymbol{X}^T \\boldsymbol{\\alpha}||^2_2$$\n",
      "\n",
      "We call $\\boldsymbol{K} = \\boldsymbol{X}\\boldsymbol{X}^T \\in \\mathbb{R}^{N \\times N}$ the Gram matrix so that we can find the minimum at\n",
      "$$0 = \\boldsymbol{K} \\boldsymbol{y} - \\boldsymbol{K}\\boldsymbol{K} \\boldsymbol{\\alpha}$$\n",
      "$$\\Leftrightarrow \\boldsymbol{\\alpha}\n",
      "= \\left( \\boldsymbol{K}\\boldsymbol{K} \\right)^{-1} \\boldsymbol{K} \\boldsymbol{y}\n",
      "= \\boldsymbol{K}^{-1} \\boldsymbol{y}\n",
      "= \\boldsymbol{K} \\boldsymbol{y}$$\n",
      "\n",
      "Instead of the *linear kernel* $k(\\boldsymbol{x}, \\boldsymbol{x}') = \\boldsymbol{x}^T \\boldsymbol{x}'$, we can use any Mercer kernel to build the Gram matrix. The intuition behind this is that we use another definition of similarity when we use another kernel."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from sklearn.metrics.pairwise import pairwise_kernels\n",
      "\n",
      "\n",
      "class KernelRegression(BaseEstimator, RegressorMixin):\n",
      "    def __init__(self, kernel, **kernel_args):\n",
      "        self.kernel = kernel\n",
      "        self.kernel_args = kernel_args\n",
      "    def fit(self, X, y):\n",
      "        self.X = X\n",
      "        K = pairwise_kernels(self.X, metric=self.kernel, **self.kernel_args)\n",
      "        self.alpha = np.linalg.pinv(K).dot(y)\n",
      "        return self\n",
      "    def predict(self, X):\n",
      "        K_star = pairwise_kernels(X, self.X, metric=self.kernel, **self.kernel_args)\n",
      "        return K_star.dot(self.alpha)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "x, X, y_true, y = generate_data(101, 0.3, 0)\n",
      "plt.plot(x, y_true)\n",
      "plt.scatter(x, y)\n",
      "kernreg = KernelRegression(\"linear\").fit(X, y)\n",
      "plt.plot(x, kernreg.predict(X))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "x, X, y_true, y = generate_data(101, 0.3, 0)\n",
      "plt.plot(x, y_true)\n",
      "plt.scatter(x, y)\n",
      "kernreg = KernelRegression(\"rbf\", gamma=10.0).fit(X, y)\n",
      "plt.plot(x, kernreg.predict(X))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "Kernel Ridge Regression"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "We start from ridge regression\n",
      "\n",
      "$$\\arg \\min_\\boldsymbol{w} \\frac{1}{2} || \\boldsymbol{y} - \\boldsymbol{X} \\boldsymbol{w} ||^2_2 + \\frac{\\gamma}{2} \\boldsymbol{w}^T \\boldsymbol{w}.$$\n",
      "\n",
      "and replace again\n",
      "\n",
      "$$\\boldsymbol{w} = \\boldsymbol{X}^T \\boldsymbol{\\alpha},$$\n",
      "\n",
      "so that\n",
      "\n",
      "$$\\boldsymbol{w}^T \\boldsymbol{w} = \\left( \\boldsymbol{X}^T \\boldsymbol{\\alpha} \\right)^T \\left( \\boldsymbol{X}^T \\boldsymbol{\\alpha} \\right) = \\boldsymbol{\\alpha}^T \\boldsymbol{X} \\boldsymbol{X}^T \\boldsymbol{\\alpha} = \\boldsymbol{\\alpha}^T \\boldsymbol{K} \\boldsymbol{\\alpha},$$\n",
      "\n",
      "which results in a modification of the dual problem\n",
      "\n",
      "$$\\arg \\min_\\boldsymbol{w} \\frac{1}{2} || \\boldsymbol{y} - \\boldsymbol{K} \\boldsymbol{\\alpha} ||^2_2 + \\frac{\\gamma}{2} \\boldsymbol{\\alpha}^T \\boldsymbol{K} \\boldsymbol{\\alpha}.$$"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "$$0\n",
      "= \\boldsymbol{K} \\left( \\boldsymbol{y} - \\boldsymbol{K} \\boldsymbol{\\alpha} \\right) + \\gamma \\boldsymbol{K} \\boldsymbol{\\alpha}\n",
      "= \\boldsymbol{K} \\boldsymbol{y} - \\boldsymbol{K} \\boldsymbol{K} \\boldsymbol{\\alpha} + \\gamma \\boldsymbol{K} \\boldsymbol{\\alpha}$$\n",
      "$$\\Leftrightarrow \\boldsymbol{K} \\boldsymbol{y} = \\boldsymbol{K} \\boldsymbol{K} \\boldsymbol{\\alpha} - \\gamma \\boldsymbol{K} \\boldsymbol{\\alpha}$$\n",
      "$$\\Leftrightarrow \\boldsymbol{y} = \\boldsymbol{K} \\boldsymbol{\\alpha} - \\gamma \\boldsymbol{I} \\boldsymbol{\\alpha}$$\n",
      "$$\\Leftrightarrow \\left(\\boldsymbol{K} - \\gamma \\boldsymbol{I} \\right)^{-1} \\boldsymbol{y} = \\boldsymbol{\\alpha}$$"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "class KernelRidgeRegression(BaseEstimator, RegressorMixin):\n",
      "    def __init__(self, kernel, lmbda, **kernel_args):\n",
      "        self.kernel = kernel\n",
      "        self.lmbda = lmbda\n",
      "        self.kernel_args = kernel_args\n",
      "    def fit(self, X, y):\n",
      "        self.X = X\n",
      "        K = pairwise_kernels(self.X, metric=self.kernel, **self.kernel_args)\n",
      "        self.alpha = np.linalg.pinv(K + self.lmbda * np.eye(X.shape[0])).dot(y)\n",
      "        return self\n",
      "    def predict(self, X):\n",
      "        K_star = pairwise_kernels(X, self.X, metric=self.kernel, **self.kernel_args)\n",
      "        return K_star.dot(self.alpha)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "x, X, y_true, y = generate_data(101, 0.3, 0)\n",
      "plt.plot(x, y_true)\n",
      "plt.scatter(x, y)\n",
      "kernridgereg = KernelRidgeRegression(\"rbf\", lmbda=1.0, gamma=10.0).fit(X, y)\n",
      "plt.plot(x, kernridgereg.predict(X))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "Gaussian Process Regression"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Kernels that are used for GPR (covariance functions) usually have many more hyperparameters than kernels that we have been looking at before. However, the hyperparameter optimization of GPR is much better than other methods (but more computationally complex).\n",
      "\n",
      "The log-likelihood\n",
      "$$\\log p(\\boldsymbol{y}|\\boldsymbol{X}) = - \\frac{1}{2} \\boldsymbol{y} \\boldsymbol{K}^{-1} \\boldsymbol{y} - \\frac{1}{2} \\log |\\boldsymbol{K}| - \\frac{N}{2} \\log (2 \\pi)$$\n",
      "will be minimized with respect to the kernel's hyperparameters $\\boldsymbol{\\theta}$. The derivative can be computed as\n",
      "$$\\frac{\\partial}{\\partial \\theta_j} \\log p(\\boldsymbol{y}|\\boldsymbol{X}) = \\frac{1}{2} tr \\left( (\\boldsymbol{\\alpha} \\boldsymbol{\\alpha}^T - \\boldsymbol{K}^{-1}) \\frac{\\partial \\boldsymbol{K}}{\\partial \\theta_j} \\right)$$"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from sklearn.metrics.pairwise import manhattan_distances\n",
      "from scipy.optimize import fmin_l_bfgs_b\n",
      "\n",
      "\n",
      "def squared_exponential(X, Y=None, theta=None):\n",
      "    D = manhattan_distances(X, Y, sum_over_features=False) ** 2\n",
      "    n_samples_X = X.shape[0]\n",
      "    n_samples_Y = D.shape[0] / n_samples_X\n",
      "    l = theta[np.newaxis, 1:]\n",
      "    sigma = np.log(theta[0] ** 2)\n",
      "    K_sqexp = np.exp(-np.sum(l * D, axis=1)).reshape(n_samples_X, n_samples_Y)\n",
      "    return K_sqexp + sigma ** 2 * np.eye(X.shape[1])\n",
      "\n",
      "\n",
      "def compute_alpha(K, y, log_det=True):\n",
      "    try:\n",
      "        L = np.linalg.cholesky(K)\n",
      "        alpha = np.linalg.solve(L.T, np.linalg.solve(L, y))\n",
      "        if log_det:\n",
      "            # 0.5 * log(|K|) = trace(log(L))\n",
      "            log_K_det = np.trace(np.log(L))\n",
      "            return alpha, log_K_det\n",
      "        else:\n",
      "            return alpha\n",
      "    except:\n",
      "        K_inv = np.linalg.pinv(K)\n",
      "        alpha = K_inv.dot(y)\n",
      "        if log_det:\n",
      "            log_K_det = 0.5 * np.linalg.det(K + 1e-10)\n",
      "            return alpha, log_K_det\n",
      "        else:\n",
      "            return alpha\n",
      "\n",
      "\n",
      "#def squared_exponential_der(X, Y=None, theta=None):\n",
      "#    D = manhattan_distances(X, Y, sum_over_features=False) ** 2\n",
      "#    return -np.sum(theta[np.newaxis] * Dnp.exp(-np.sum(theta[np.newaxis] * D, axis=1))\n",
      "\n",
      "\n",
      "class GaussianProcessRegression(BaseEstimator, RegressorMixin):\n",
      "    def __init__(self, verbose=0):\n",
      "        self.verbose = verbose\n",
      "    def fit(self, X, y):\n",
      "        self.n_samples, n_features = X.shape\n",
      "        self.X = X\n",
      "        self.y = y\n",
      "        res = fmin_l_bfgs_b(\n",
      "            self._objective, 0.1 * np.ones(n_features + 1), approx_grad=True,\n",
      "            bounds=[(1e-2, 1e10) for _ in range(n_features + 1)])\n",
      "        self.theta_ = res[0]\n",
      "        K = squared_exponential(self.X, theta=self.theta_)\n",
      "        self.alpha = compute_alpha(K, self.y, log_det=False)\n",
      "        return self\n",
      "    def _objective(self, theta):\n",
      "        K = squared_exponential(self.X, theta=theta)\n",
      "        alpha, log_K_det = compute_alpha(K, self.y)\n",
      "        log_likelihood = (-0.5 * self.y.dot(alpha) - log_K_det\n",
      "                          -0.5 * self.n_samples * np.log(2 * np.pi))\n",
      "\n",
      "        if self.verbose:\n",
      "            print(\"Cost: %g\" % log_likelihood)\n",
      "        #alpha = K_inv.dot(self.y)\n",
      "        return log_likelihood\n",
      "    def predict(self, X):\n",
      "        K_star = squared_exponential(X, self.X, theta=self.theta_)\n",
      "        return K_star.dot(self.alpha)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "erx, X, y_true, y = generate_data(101, 0.3, 0)\n",
      "plt.plot(x, y_true)\n",
      "plt.scatter(x, y)\n",
      "gpreg = GaussianProcessRegression(verbose=1).fit(X, y)\n",
      "plt.plot(x, gpreg.predict(X))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print gpreg.theta_"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}