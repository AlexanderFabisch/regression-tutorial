{
 "metadata": {
  "name": ""
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%pylab inline"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "Linear Regression"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "**Model:** We assume that there is some latent function $f: \\mathbb{R}^D \\rightarrow \\mathbb{R}$. We observe samples $(\\boldsymbol{x}_n, y_n)$ with $f(\\boldsymbol{x}_n) + \\epsilon_n = y_n$. Since we assume that $f$ is a linear function of the form $f(\\boldsymbol{x}_n) = \\boldsymbol{w} \\boldsymbol{x} + b$, we will learn by minimizing\n",
      "\n",
      "$$\\arg \\min_\\boldsymbol{w} \\frac{1}{2} || \\boldsymbol{y} - \\boldsymbol{X} \\boldsymbol{w} ||^2_2.$$"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "$$0 = \\boldsymbol{X}^T \\left( \\boldsymbol{y} - \\boldsymbol{X} \\boldsymbol{w}^T \\right) \\boldsymbol{X} = \\boldsymbol{X}^T \\boldsymbol{y} - \\boldsymbol{X}^T \\boldsymbol{X} \\boldsymbol{w}$$\n",
      "$$\\boldsymbol{X}^T \\boldsymbol{X} \\boldsymbol{w} = \\Leftrightarrow \\boldsymbol{X}^T \\boldsymbol{y}$$\n",
      "$$\\Leftrightarrow \\boldsymbol{w} = \\left( \\boldsymbol{X}^T \\boldsymbol{X} \\right)^{-1} \\boldsymbol{X}^T \\boldsymbol{y}$$"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from sklearn.base import BaseEstimator, RegressorMixin\n",
      "\n",
      "\n",
      "class LinearRegression(BaseEstimator, RegressorMixin):\n",
      "    def fit(self, X, y):\n",
      "        n_samples = X.shape[0]\n",
      "        X_bias = np.hstack((np.ones((n_samples, 1)), X))\n",
      "        self.w = np.linalg.inv(X_bias.T.dot(X_bias)).dot(X_bias.T).dot(y)\n",
      "        return self\n",
      "    def predict(self, X):\n",
      "        n_samples = X.shape[0]\n",
      "        X_bias = np.hstack((np.ones((n_samples, 1)), X))\n",
      "        return X_bias.dot(self.w)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "n_samples = 101\n",
      "x = np.linspace(0, 1, n_samples)\n",
      "X = x[:, np.newaxis]\n",
      "y_true = np.cos(2 * np.pi * x)\n",
      "y = y_true + 0.2 * np.random.randn(n_samples)\n",
      "\n",
      "plt.plot(x, y_true)\n",
      "plt.scatter(x, y)\n",
      "linreg = LinearRegression().fit(X, y)\n",
      "plt.plot(x, linreg.predict(X))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "Polynomial Regression"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from sklearn.preprocessing import PolynomialFeatures\n",
      "\n",
      "\n",
      "class PolynomialRegression(BaseEstimator, RegressorMixin):\n",
      "    def __init__(self, degree):\n",
      "        self.degree = degree\n",
      "    def fit(self, X, y):\n",
      "        self.poly = PolynomialFeatures(degree=self.degree).fit(X)\n",
      "        X_poly = self.poly.transform(X)\n",
      "        self.w = np.linalg.inv(X_poly.T.dot(X_poly)).dot(X_poly.T).dot(y)\n",
      "        return self\n",
      "    def predict(self, X):\n",
      "        X_poly = self.poly.transform(X)\n",
      "        return X_poly.dot(self.w)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "n_samples = 101\n",
      "x = np.linspace(0, 1, n_samples)\n",
      "X = x[:, np.newaxis]\n",
      "y_true = np.cos(2 * np.pi * x)\n",
      "y = y_true + np.random.randn(n_samples)\n",
      "\n",
      "plt.plot(x, y_true)\n",
      "plt.scatter(x, y)\n",
      "polyreg = PolynomialRegression(3).fit(X, y)\n",
      "plt.plot(x, polyreg.predict(X))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "Kernel Regression"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from sklearn.metrics.pairwise import pairwise_kernels\n",
      "\n",
      "\n",
      "class KernelRegression(BaseEstimator, RegressorMixin):\n",
      "    def __init__(self, kernel, **kernel_args):\n",
      "        self.kernel = kernel\n",
      "        self.kernel_args = kernel_args\n",
      "    def fit(self, X, y):\n",
      "        self.X = X\n",
      "        K = pairwise_kernels(self.X, metric=self.kernel, **self.kernel_args)\n",
      "        self.Kinvy = np.linalg.pinv(K).dot(y)\n",
      "        return self\n",
      "    def predict(self, X):\n",
      "        K_star = pairwise_kernels(X, self.X, metric=self.kernel, **self.kernel_args)\n",
      "        return K_star.dot(self.Kinvy)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "n_samples = 101\n",
      "x = np.linspace(0, 1, n_samples)\n",
      "X = x[:, np.newaxis]\n",
      "y_true = np.cos(2 * np.pi * x)\n",
      "y = y_true + np.random.randn(n_samples)\n",
      "\n",
      "plt.plot(x, y_true)\n",
      "plt.scatter(x, y)\n",
      "kernreg = KernelRegression(\"rbf\", gamma=0.1).fit(X, y)\n",
      "plt.plot(x, kernreg.predict(X))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "$$\\left( \\boldsymbol{X}^T \\boldsymbol{X} \\right)^{-1} \\boldsymbol{X}^T = \\boldsymbol{X}^T \\left( \\boldsymbol{X} \\boldsymbol{X}^T \\right)^{-1}$$"
     ]
    }
   ],
   "metadata": {}
  }
 ]
}