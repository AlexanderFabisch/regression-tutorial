{
 "metadata": {
  "name": "",
  "signature": "sha256:9814dc83e778a57b3330ae85fa1bc895abfdfdbc5c851c14c59ef34be7d0c101"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%pylab inline"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from sklearn.utils import check_random_state\n",
      "\n",
      "def generate_data(n_samples, sigma, random_state=None):\n",
      "    random_state = check_random_state(random_state)\n",
      "    x = np.linspace(0, 1, n_samples)\n",
      "    X = x[:, np.newaxis]\n",
      "    y_true = np.cos(2 * np.pi * x)\n",
      "    y = y_true + sigma * random_state.randn(n_samples)\n",
      "    return x, X, y_true, y"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "Linear Regression"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "**Model:** We assume that there is some latent function $f: \\mathbb{R}^D \\rightarrow \\mathbb{R}$. We observe samples $(\\boldsymbol{x}_n, y_n)$ with $f(\\boldsymbol{x}_n) + \\epsilon_n = y_n$ and $\\epsilon_n \\sim \\mathcal{N}(0, \\sigma^2)$. Since we assume that $f$ is a linear function of the form $f(\\boldsymbol{x}_n) = \\boldsymbol{w} \\boldsymbol{x}$ ($\\boldsymbol{x}$ will be extended by 1 so that $\\boldsymbol{w}$ incorporates a bias), we will learn by minimizing\n",
      "\n",
      "$$\\arg \\min_\\boldsymbol{w} \\frac{1}{2} || \\boldsymbol{y} - \\boldsymbol{X} \\boldsymbol{w} ||^2_2.$$"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "We can find a solution for the vector $\\boldsymbol{w}$ by setting the derivative of the objective function to 0 and solving for $\\boldsymbol{w}$.\n",
      "\n",
      "\\begin{eqnarray*}\n",
      "&& 0 = \\boldsymbol{X}^T \\left( \\boldsymbol{y} - \\boldsymbol{X} \\boldsymbol{w} \\right) \\boldsymbol{X} = \\boldsymbol{X}^T \\boldsymbol{y} - \\boldsymbol{X}^T \\boldsymbol{X} \\boldsymbol{w}\\\\\n",
      "&\\Leftrightarrow& \\boldsymbol{X}^T \\boldsymbol{X} \\boldsymbol{w} = \\boldsymbol{X}^T \\boldsymbol{y}\\\\\n",
      "&\\Leftrightarrow& \\boldsymbol{w} = \\left( \\boldsymbol{X}^T \\boldsymbol{X} \\right)^{-1} \\boldsymbol{X}^T \\boldsymbol{y}\n",
      "\\end{eqnarray*}\n",
      "\n",
      "(Now we actually have to prove that the second derivative is greater than 0.)"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from sklearn.base import BaseEstimator, RegressorMixin\n",
      "\n",
      "\n",
      "class LinearRegression(BaseEstimator, RegressorMixin):\n",
      "    def fit(self, X, y):\n",
      "        n_samples = X.shape[0]\n",
      "        X_bias = np.hstack((np.ones((n_samples, 1)), X))\n",
      "        self.w = np.linalg.inv(X_bias.T.dot(X_bias)).dot(X_bias.T).dot(y)\n",
      "        return self\n",
      "    def predict(self, X):\n",
      "        n_samples = X.shape[0]\n",
      "        X_bias = np.hstack((np.ones((n_samples, 1)), X))\n",
      "        return X_bias.dot(self.w)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "x, X, y_true, y = generate_data(101, 0.3, 0)\n",
      "plt.plot(x, y_true)\n",
      "plt.scatter(x, y)\n",
      "linreg = LinearRegression().fit(X, y)\n",
      "plt.plot(x, linreg.predict(X))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "Polynomial Regression"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "We can approximate nonlinear functions with linear regression by generating nonlinear features $\\phi(\\boldsymbol{x})$."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from sklearn.preprocessing import PolynomialFeatures\n",
      "\n",
      "\n",
      "class PolynomialRegression(BaseEstimator, RegressorMixin):\n",
      "    def __init__(self, degree):\n",
      "        self.degree = degree\n",
      "    def fit(self, X, y):\n",
      "        self.poly = PolynomialFeatures(degree=self.degree).fit(X)\n",
      "        X_poly = self.poly.transform(X)\n",
      "        self.w = np.linalg.pinv(X_poly.T.dot(X_poly)).dot(X_poly.T).dot(y)\n",
      "        return self\n",
      "    def predict(self, X):\n",
      "        X_poly = self.poly.transform(X)\n",
      "        return X_poly.dot(self.w)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "x, X, y_true, y = generate_data(101, 0.3, 0)\n",
      "plt.plot(x, y_true)\n",
      "plt.scatter(x, y)\n",
      "polyreg = PolynomialRegression(50).fit(X, y)\n",
      "plt.plot(x, polyreg.predict(X))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "Ridge Regression"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "To reduce overfitting, we minimize\n",
      "\n",
      "$$\\arg \\min_\\boldsymbol{w} \\frac{1}{2} || \\boldsymbol{y} - \\boldsymbol{X} \\boldsymbol{w} ||^2_2 + \\frac{\\gamma}{2} \\boldsymbol{w}^T \\boldsymbol{w},$$\n",
      "\n",
      "which means that we set a prior for $\\boldsymbol{w}$."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "\\begin{eqnarray*}\n",
      "&& 0\n",
      "= \\boldsymbol{X}^T \\left( \\boldsymbol{y} - \\boldsymbol{X} \\boldsymbol{w}^T \\right) \\boldsymbol{X} + \\gamma \\boldsymbol{w}\n",
      "= \\boldsymbol{X}^T \\boldsymbol{y} - \\boldsymbol{X}^T \\boldsymbol{X} \\boldsymbol{w} + \\gamma \\boldsymbol{w}\\\\\n",
      "&\\Leftrightarrow& \\boldsymbol{X}^T \\boldsymbol{X} \\boldsymbol{w} + \\gamma \\boldsymbol{w} = \\boldsymbol{X}^T \\boldsymbol{y}\\\\\n",
      "&\\Leftrightarrow& \\boldsymbol{w} = \\left( \\boldsymbol{X}^T \\boldsymbol{X} + \\gamma \\boldsymbol{I} \\right)^{-1} \\boldsymbol{X}^T \\boldsymbol{y}\n",
      "\\end{eqnarray*}"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "class PolynomialRidgeRegression(BaseEstimator, RegressorMixin):\n",
      "    def __init__(self, degree, gamma):\n",
      "        self.degree = degree\n",
      "        self.gamma = gamma\n",
      "    def fit(self, X, y):\n",
      "        self.poly = PolynomialFeatures(degree=self.degree).fit(X)\n",
      "        X_poly = self.poly.transform(X)\n",
      "        self.w = (np.linalg.inv(X_poly.T.dot(X_poly) + self.gamma *\n",
      "                                np.eye(X_poly.shape[1])).dot(X_poly.T).dot(y))\n",
      "        return self\n",
      "    def predict(self, X):\n",
      "        X_poly = self.poly.transform(X)\n",
      "        return X_poly.dot(self.w)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "x, X, y_true, y = generate_data(101, 0.3, 0)\n",
      "plt.plot(x, y_true)\n",
      "plt.scatter(x, y)\n",
      "polyridgereg = PolynomialRidgeRegression(20, 0.1).fit(X, y)\n",
      "plt.plot(x, polyridgereg.predict(X))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "Kernel Regression"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "We can express $\\boldsymbol{w}$ as a weighted sum of the training data\n",
      "$$\\boldsymbol{w} = \\boldsymbol{X}^T \\boldsymbol{\\alpha}$$\n",
      "\n",
      "Which leads to the **dual problem**\n",
      "$$\\arg \\min_\\boldsymbol{\\alpha} \\frac{1}{2} || \\boldsymbol{y} - \\boldsymbol{X} \\boldsymbol{X}^T \\boldsymbol{\\alpha}||^2_2$$\n",
      "\n",
      "We call $\\boldsymbol{K} = \\boldsymbol{X}\\boldsymbol{X}^T \\in \\mathbb{R}^{N \\times N}$ the Gram matrix so that we can find the minimum at\n",
      "\\begin{eqnarray*}\n",
      "&& 0 = \\boldsymbol{K} \\boldsymbol{y} - \\boldsymbol{K}\\boldsymbol{K} \\boldsymbol{\\alpha}\\\\\n",
      "&\\Leftrightarrow& \\boldsymbol{\\alpha}\n",
      "= \\left( \\boldsymbol{K}\\boldsymbol{K} \\right)^{-1} \\boldsymbol{K} \\boldsymbol{y}\n",
      "= \\boldsymbol{K}^{-1} \\boldsymbol{y}\n",
      "= \\boldsymbol{K} \\boldsymbol{y}\n",
      "\\end{eqnarray*}\n",
      "\n",
      "Instead of the *linear kernel* $k(\\boldsymbol{x}, \\boldsymbol{x}') = \\boldsymbol{x}^T \\boldsymbol{x}'$, we can use any Mercer kernel to build the Gram matrix. The intuition behind this is that we use another definition of similarity when we use another kernel."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from sklearn.metrics.pairwise import pairwise_kernels\n",
      "\n",
      "\n",
      "class KernelRegression(BaseEstimator, RegressorMixin):\n",
      "    def __init__(self, kernel, **kernel_args):\n",
      "        self.kernel = kernel\n",
      "        self.kernel_args = kernel_args\n",
      "    def fit(self, X, y):\n",
      "        self.X = X\n",
      "        K = pairwise_kernels(self.X, metric=self.kernel, **self.kernel_args)\n",
      "        self.alpha = np.linalg.pinv(K).dot(y)\n",
      "        return self\n",
      "    def predict(self, X):\n",
      "        K_star = pairwise_kernels(X, self.X, metric=self.kernel, **self.kernel_args)\n",
      "        return K_star.dot(self.alpha)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "x, X, y_true, y = generate_data(101, 0.3, 0)\n",
      "plt.plot(x, y_true)\n",
      "plt.scatter(x, y)\n",
      "kernreg = KernelRegression(\"linear\").fit(X, y)\n",
      "plt.plot(x, kernreg.predict(X))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "x, X, y_true, y = generate_data(101, 0.3, 0)\n",
      "plt.plot(x, y_true)\n",
      "plt.scatter(x, y)\n",
      "kernreg = KernelRegression(\"rbf\", gamma=10.0).fit(X, y)\n",
      "plt.plot(x, kernreg.predict(X))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "Kernel Ridge Regression"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "We start from ridge regression\n",
      "\n",
      "$$\\arg \\min_\\boldsymbol{w} \\frac{1}{2} || \\boldsymbol{y} - \\boldsymbol{X} \\boldsymbol{w} ||^2_2 + \\frac{\\gamma}{2} \\boldsymbol{w}^T \\boldsymbol{w}.$$\n",
      "\n",
      "and replace again\n",
      "\n",
      "$$\\boldsymbol{w} = \\boldsymbol{X}^T \\boldsymbol{\\alpha},$$\n",
      "\n",
      "so that\n",
      "\n",
      "$$\\boldsymbol{w}^T \\boldsymbol{w} = \\left( \\boldsymbol{X}^T \\boldsymbol{\\alpha} \\right)^T \\left( \\boldsymbol{X}^T \\boldsymbol{\\alpha} \\right) = \\boldsymbol{\\alpha}^T \\boldsymbol{X} \\boldsymbol{X}^T \\boldsymbol{\\alpha} = \\boldsymbol{\\alpha}^T \\boldsymbol{K} \\boldsymbol{\\alpha},$$\n",
      "\n",
      "which results in a modification of the dual problem\n",
      "\n",
      "$$\\arg \\min_\\boldsymbol{w} \\frac{1}{2} || \\boldsymbol{y} - \\boldsymbol{K} \\boldsymbol{\\alpha} ||^2_2 + \\frac{\\gamma}{2} \\boldsymbol{\\alpha}^T \\boldsymbol{K} \\boldsymbol{\\alpha}.$$"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "\\begin{eqnarray*}\n",
      "&& 0\n",
      "= \\boldsymbol{K} \\left( \\boldsymbol{y} - \\boldsymbol{K} \\boldsymbol{\\alpha} \\right) + \\gamma \\boldsymbol{K} \\boldsymbol{\\alpha}\n",
      "= \\boldsymbol{K} \\boldsymbol{y} - \\boldsymbol{K} \\boldsymbol{K} \\boldsymbol{\\alpha} + \\gamma \\boldsymbol{K} \\boldsymbol{\\alpha}\\\\\n",
      "&\\Leftrightarrow& \\boldsymbol{K} \\boldsymbol{y} = \\boldsymbol{K} \\boldsymbol{K} \\boldsymbol{\\alpha} - \\gamma \\boldsymbol{K} \\boldsymbol{\\alpha}\\\\\n",
      "&\\Leftrightarrow& \\boldsymbol{y} = \\boldsymbol{K} \\boldsymbol{\\alpha} - \\gamma \\boldsymbol{I} \\boldsymbol{\\alpha}\\\\\n",
      "&\\Leftrightarrow& \\left(\\boldsymbol{K} - \\gamma \\boldsymbol{I} \\right)^{-1} \\boldsymbol{y} = \\boldsymbol{\\alpha}\n",
      "\\end{eqnarray*}"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "class KernelRidgeRegression(BaseEstimator, RegressorMixin):\n",
      "    def __init__(self, kernel, lmbda, **kernel_args):\n",
      "        self.kernel = kernel\n",
      "        self.lmbda = lmbda\n",
      "        self.kernel_args = kernel_args\n",
      "    def fit(self, X, y):\n",
      "        self.X = X\n",
      "        K = pairwise_kernels(self.X, metric=self.kernel, **self.kernel_args)\n",
      "        self.alpha = np.linalg.pinv(K + self.lmbda * np.eye(X.shape[0])).dot(y)\n",
      "        return self\n",
      "    def predict(self, X):\n",
      "        K_star = pairwise_kernels(X, self.X, metric=self.kernel, **self.kernel_args)\n",
      "        return K_star.dot(self.alpha)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "x, X, y_true, y = generate_data(101, 0.3, 0)\n",
      "plt.plot(x, y_true)\n",
      "plt.scatter(x, y)\n",
      "kernridgereg = KernelRidgeRegression(\"rbf\", lmbda=1.0, gamma=10.0).fit(X, y)\n",
      "plt.plot(x, kernridgereg.predict(X))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "Gaussian Process Regression"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Kernels that are used for GPR (covariance functions) usually have many more hyperparameters than kernels that we have been looking at before. However, the hyperparameter optimization of GPR is much better than other methods (but more computationally complex).\n",
      "\n",
      "The log-likelihood\n",
      "$$\\log p(\\boldsymbol{y}|\\boldsymbol{X}) = - \\frac{1}{2} \\boldsymbol{y} \\boldsymbol{K}^{-1} \\boldsymbol{y} - \\frac{1}{2} \\log |\\boldsymbol{K}| - \\frac{N}{2} \\log (2 \\pi)$$\n",
      "will be minimized with respect to the kernel's hyperparameters $\\boldsymbol{\\theta}$. The derivative can be computed as\n",
      "$$\\frac{\\partial}{\\partial \\theta_j} \\log p(\\boldsymbol{y}|\\boldsymbol{X}) = \\frac{1}{2} tr \\left( (\\boldsymbol{\\alpha} \\boldsymbol{\\alpha}^T - \\boldsymbol{K}^{-1}) \\frac{\\partial \\boldsymbol{K}}{\\partial \\theta_j} \\right)$$"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from scipy import linalg, optimize\n",
      "from sklearn.metrics.pairwise import manhattan_distances\n",
      "from sklearn.gaussian_process import correlation_models as correlation\n",
      "\n",
      "\n",
      "def l1_cross_distances(X):\n",
      "    n_samples, n_features = X.shape\n",
      "    n_nonzero_cross_dist = n_samples * (n_samples - 1) // 2\n",
      "    ij = np.zeros((n_nonzero_cross_dist, 2), dtype=np.int)\n",
      "    D = np.zeros((n_nonzero_cross_dist, n_features))\n",
      "    ll_1 = 0\n",
      "    for k in range(n_samples - 1):\n",
      "        ll_0 = ll_1\n",
      "        ll_1 = ll_0 + n_samples - k - 1\n",
      "        ij[ll_0:ll_1, 0] = k\n",
      "        ij[ll_0:ll_1, 1] = np.arange(k + 1, n_samples)\n",
      "        D[ll_0:ll_1] = np.abs(X[k] - X[(k + 1):n_samples])\n",
      "    return D, ij\n",
      "\n",
      "\n",
      "def kernel(X, Y=None, theta=None):\n",
      "    dx = manhattan_distances(X, Y, sum_over_features=False)\n",
      "    n_samples = X.shape[0]\n",
      "    n_eval = dx.shape[0] / n_samples\n",
      "    return correlation.squared_exponential(theta, dx).reshape(n_eval, n_samples)\n",
      "\n",
      "\n",
      "class GaussianProcessRegression(BaseEstimator, RegressorMixin):\n",
      "    def __init__(self, nugget):\n",
      "        self.nugget = nugget\n",
      "\n",
      "    def fit(self, X, y):\n",
      "        self.n_samples, self.n_features = X.shape\n",
      "        self.X = X\n",
      "        self.y = y\n",
      "        self.D, self.ij = l1_cross_distances(X)\n",
      "        self.theta_, _, par = self._optimize_theta()\n",
      "        self.__dict__.update(par)\n",
      "        return self\n",
      "\n",
      "    def predict(self, X, eval_MSE=False):\n",
      "        K = kernel(X, self.X, self.theta_)\n",
      "        y = np.dot(K, self.alpha)\n",
      "        if eval_MSE:\n",
      "            Kt = linalg.solve_triangular(self.L, K.T, lower=True)\n",
      "            MSE = np.dot(self.sigma2, (1 - (Kt ** 2).sum(axis=0))[np.newaxis])\n",
      "            MSE = np.sqrt((MSE ** 2).sum(axis=0))\n",
      "            return y, MSE\n",
      "        else:\n",
      "            return y\n",
      "\n",
      "    def reduced_likelihood(self, theta):\n",
      "        k = correlation.squared_exponential(theta, self.D)\n",
      "        K = np.eye(self.n_samples) * (1. + self.nugget)\n",
      "        K[self.ij[:, 0], self.ij[:, 1]] = k\n",
      "        K[self.ij[:, 1], self.ij[:, 0]] = k\n",
      "        try:\n",
      "            L = linalg.cholesky(K, lower=True)\n",
      "        except linalg.LinAlgError:\n",
      "            return -np.inf, {}\n",
      "        Ly = linalg.solve_triangular(L, self.y, lower=True)\n",
      "        alpha = linalg.solve_triangular(L.T, Ly)\n",
      "        sigma2 = (Ly ** 2.).sum() / self.n_samples\n",
      "        log_K_det = (np.diag(L) ** (2. / self.n_samples)).prod()\n",
      "        objective = -sigma2 * log_K_det\n",
      "        return objective, {\"L\": L, \"alpha\": alpha, \"sigma2\": sigma2}\n",
      "\n",
      "    def _optimize_theta(self):\n",
      "        def objective(log10t):\n",
      "            return -self.reduced_likelihood(theta=10 ** log10t)[0]\n",
      "\n",
      "        constraints = reduce(lambda x, y: x + y,\n",
      "                             [(lambda log10t, i=i: log10t[i] - 1e-10,\n",
      "                               lambda log10t, i=i: 1e+10 - log10t[i])\n",
      "                              for i in range(self.n_features)])\n",
      "\n",
      "        log10_optimal_theta = optimize.fmin_cobyla(\n",
      "            objective, np.log10(np.ones((1, self.n_features))), constraints, iprint=0)\n",
      "        optimal_theta = 10. ** log10_optimal_theta\n",
      "        optimal_rlf_value, optimal_par = self.reduced_likelihood(\n",
      "            theta=optimal_theta)\n",
      "\n",
      "        return optimal_theta, optimal_rlf_value, optimal_par"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "x, X, y_true, y = generate_data(101, 0.3, 0)\n",
      "plt.plot(x, y_true)\n",
      "plt.scatter(x, y)\n",
      "gpreg = GaussianProcessRegression(nugget=0.3).fit(X, y)\n",
      "y_pred, y_mse = gpreg.predict(X, eval_MSE=True)\n",
      "plt.fill_between(x, y_pred - np.sqrt(y_mse), y_pred + np.sqrt(y_mse), alpha=0.3)\n",
      "plt.plot(x, y_pred)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}